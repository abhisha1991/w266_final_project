# Analysis of ShapSum Results on New Summarization Models

Submission by *Carolina Arriaga, Ayman, Abhi Sharma*

Winter 2021 | UC Berkeley

## Introduction

In order to apply the ShapSum framework to see how well the score predictions would generalize to unseen summaries, the team took the predictive scoring model built in the paper and used it to predict scores for each dimension (fluency, consistency, coherence, relevance) for novel summaries. The motivation was to understand the difference in dimension scores between extractive and abstractive approaches of summarization for new summaries. These summaries were generated by standard baseline and SOTA models selected by the team.

The team was also interested in conducting an analysis on summary quality vs summary length. In the [SummEval](https://arxiv.org/abs/2007.12626) paper, the authors generated summaries from their 16 models without any summary length enforcement. The team investigated whether summary quality (say fluency) would be impacted if the model was forced to output summaries subject to length constraints. The team evaluated summaries from the same model on the same article with varying lengths - 20, 30, 45, 60 - and tried to draw inferences.

Below, we present the models employed and results.

## Models

The team chose 4 abstractive and 4 extractive models to generate summaries. All models can be found in the [Models notebook](https://github.com/abhisha1991/w266_final_project/blob/main/code/Models.ipynb) under the code folder.

For extractive models, the team selected:
1. TopN sentences (including variants of shuffled sentences and top-bottom sentences)
2. TF-IDF Vectorizer
3. Cossim with Text Rank with Glove Embeddings
4. BERT Extractive Summarizer

For abstractive models, the team selected:
1. BART - Large CNN (Facebook)
2. Pegasus - Google-XSum
3. T5 - Base
4. GPT-3 (DaVinci engine with vanilla `notes_summary` and `tldr` tasks)

Note that the team did not pre-train (fine-tune) any of these models on CNN/DM and we compared the summaries generated directly. This was done to evaluate baseline summary quality of each of the above models without fine tuning. It was also not possible to fine-tune certain models like GPT-3 (since the team doesn't have access to proprietary model files), so it would be unfair to fine-tune one model (say Pegasus) and not another. Future efforts can involve fine tuning but will require significant compute to do so.

## Results

At a high level, we observe that extractive models score higher on fluency and consistency dimensions, when compared to abstractive models. Coherence and relevance are similarly distributed between the 2 model types.

#### Fluency Comparison Between Abstractive and Extractive Approaches to Summarization
The below image shows that extractive models tend to be more fluent. The plausible justification for this is that extractive models are largely borrowing from the source article. Since the source article is fluent, extractive summaries would tend to be more fluent as well.
![img](https://github.com/abhisha1991/w266_final_project/blob/main/analysis/abs_ext_fluency.png)

#### Consistency Comparison Between Abstractive and Extractive Approaches to Summarization
The below image highlights that extractive models are more consistent - in the sense that they tend to more closely entail the source article and do not "make up" facts in the summary. This is understandable because abstractive techniques tend to generate more novel sentences and structures from their language models and sometimes may make up words or phrases that cannot be justified by the source context.
![img](https://github.com/abhisha1991/w266_final_project/blob/main/analysis/abs_ext_consistency.png)

Both relevance and coherence don't reveal any stark differences between the 2 model types. 

We show the distribution of scores for relevance below

![img](https://github.com/abhisha1991/w266_final_project/blob/main/analysis/abs_ext_relevance.png)

We show the distribution of scores for coherence below

![img](https://github.com/abhisha1991/w266_final_project/blob/main/analysis/abs_ext_coherence.png)

### Example Outputs from Models
Below, we see a few samples of good vs bad summaries in each dimension

#### Poor Coherence

Reference:
```
"There has been an unusually high number of sea lions stranded since January," NOAA representative says .
The speculation is mothers are having difficulty finding food, leaving pups alone too long or malnourished .
```

Prediction:
```
since January 2015, every month has set a record in "strandings," mostly sea-lion pup. 1,800 [solo] dogs have been responded to this year alone; more than 2004-12 combined -- and it is only mid March!
```

Model: 

t5-base

This sentence has been penalized for coherence because the second sentence talks about dogs and not sea lions, which is the topic of the article. Moreover, the generated summary fails to capture the essence of why may the sea lions be getting stranded - instead the prediction dives nto more numbers and specifics, which is relevant but it distracts from the uber summary. Full context [here](https://github.com/abhisha1991/w266_final_project/blob/main/analysis/contexts/sealion.txt).

#### Poor Fluency

Reference:
```
Accident happens in Santa Ynez, California, near where Crosby lives .
The jogger suffered multiple fractures; his injuries are not believed to be life-threatening .
```

Prediction: 
```
David Crosby, an American singer-songwriter, was driving at approximately 50 mph when he struck a jogger with his car.
Liar 

The Truth: David Crosby, an American singer-songwriter
```

Model:

gpt3-notes_summary_davinci

We notice that this summary was penalized for fluency because GPT-3 decided to repeat the introduction about David Crosby. It also inserted declarations like "Liar" and "The Truth" - which break fluency and are not very grammatical. Full context [here](https://github.com/abhisha1991/w266_final_project/blob/main/analysis/contexts/crosby.txt).

#### Poor Consistency

Reference:
```Clinton Foundation has taken money from foreign governments .
Bill Clinton:  "I believe we have done a lot more good than harm"
```

Prediction:
```
The Clinton Foundation is a charity that does good works, but it also takes money from countries that do
```

Model:

gpt3-tldr_davinci

The phrase: _"The Clinton Foundation is a charity that does good works"_ in the prediction shows that GPT-3 was hallucinating outputs. This text never existed in the context, linked [here](https://github.com/abhisha1991/w266_final_project/blob/main/analysis/contexts/bill_clinton.txt)

#### Poor Relevance

Reference:
```
It's unclear what will become of the other kidnapped Assyrians .
Syrian Observatory: ISIS has executed nearly 2,000 people since June .
```

Prediction:
```
Islamic State in Iraq and the Levant (ISIS) has released 19 Assyrian Christian hostages, an activist group says.
```

Model:
  
pegasus-google-xsum

This summary failed to capture the main highlight - that ISIS had already killed over 2000 people and it is unclear what will happen to the other kidnapped Assyrians. Full context [here](https://github.com/abhisha1991/w266_final_project/blob/main/analysis/contexts/isis.txt)

### Summary Length vs Summary Quality

As discussed above, we generate summaries for each of our selected models against different articles from the CNN/DM validation set. We chose different constrained summary length settings - 20, 30, 45, 60 words, such that we produced different summary lengths using the same model on the same article and we repeated this for several articles.

We plot the relationship between summary length and summary quality. We observe that there is very little relationship we can establish looking at the graphs. Below we have plotted a graph that appeared to show some pattern of summary quality when varying length, but it is still inappropriate to draw any conclusions from this. Note that all graphs for all Model-DimensionScore combinations are available in the [zip file](https://github.com/abhisha1991/w266_final_project/blob/main/analysis/varying_length_summ_scores.zip) in the analysis folder. All grpahs have been smoothened for plotting purposes.

![img](https://github.com/abhisha1991/w266_final_project/blob/main/analysis/t5_consistency.png)

The above is a sample graph (smoothened) showing the consistency relationship of t5-base model outputs against summary length. We see that there is no strong relationship observed. The same is true for other model - dimension combinations in the experiment we conducted. We will need to perform a more thorough analysis with even larger samples and even broader length settings to come to a firm conclusion.

