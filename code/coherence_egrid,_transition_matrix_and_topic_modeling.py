# -*- coding: utf-8 -*-
"""Coherence - EGRID, Transition Matrix and Topic Modeling

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EIs6EsSaDRXTAWe610v4KjtNvXdx4R1j

# Install of Dependencies
"""

!pip install transformers
# need to install CoreNLP
!wget http://nlp.stanford.edu/software/stanford-corenlp-latest.zip
!sleep 60

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

# Commented out IPython magic to ensure Python compatibility.

!unzip stanford-corenlp-latest.zip
# %cd /content/stanford-corenlp-4.3.2
!for file in `find . -name "*.jar"`; do export CLASSPATH="$CLASSPATH:`realpath $file`"; done
import os
os.environ["CORENLP_HOME"] = '/content/stanford-corenlp-4.3.2'

# Commented out IPython magic to ensure Python compatibility.
# Install other dependencies
# %cd /content
!pip install nltk
!pip install scikit-learn
!pip install stanza
!pip install datasets==1.0.2
!pip install transformers
!pip install gensim

# Commented out IPython magic to ensure Python compatibility.
# Install EGrid Implementation
!git clone https://github.com/caroarriaga/coheoka.git
# %cd coheoka/coheoka

import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')

# Use server for CoreNLP
import stanza
from stanza.server import CoreNLPClient
import time

# Read data
from __future__ import print_function, division
import json
import itertools
from itertools import product
from pprint import pprint

# Modeling
from nltk import sent_tokenize
from sklearn import preprocessing
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
from sklearn import utils
from sklearn.utils import shuffle
import matplotlib.pyplot as plt
from scipy.stats import kendalltau as tau
import numpy as np

import evaluator as eval
from ranking import transform_pairwise

import pandas as pd
from entity_grid import EntityGrid, Constants
from entity_transition import EntityTransition, TransitionMatrix
from coherence_probability import CoherenceProbability, ProbabilityVector

from sklearn.metrics import classification_report

# Topic modeling
import datasets
import transformers

import gensim
from gensim import models, corpora, similarities
from gensim.models import CoherenceModel
from gensim.corpora import Dictionary
from gensim.models import LdaModel
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk import FreqDist
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import random
import time

from scipy.stats import entropy
from numpy import dot
from numpy.linalg import norm

"""# Load the data

## Annotated data

We extracted from the expert annotated dataset all of the decoded summaries and calculated the mean expert score for each dimension of interest.

Dimensions of interes: coherence, consistency, fluency and relevance.
"""

# need upload file (drive/data)
outputs = pd.read_csv('/content/data_scored.csv')

outputs.columns

"""## Original documents

We also tracked the original documents based on the annotators dataset which included the id reference to the original document.
"""

# all data
all_articles = datasets.load_dataset("cnn_dailymail", "3.0.0")

# train_set = pd.DataFrame(all_articles['train'])
# test_set = pd.DataFrame(all_articles['test'])
test_set = pd.DataFrame(all_articles['validation'])
test_set.head()

def get_cnndm_by_id(dataset, id, return_article_only=True):
  id = id.replace('dm-test-', '')
  id = id.replace('dm-train-', '')
  id = id.replace('dm-dev-', '')
  id = id.replace('dm-val-', '')
  
  id = id.replace('cnn-test-', '')
  id = id.replace('cnn-train-', '')
  id = id.replace('cnn-dev-', '')
  id = id.replace('cnn-val-', '')
  try:
    highlight = dataset.filter(lambda x: x['id'] == id)['highlights'][0]
    article = dataset.filter(lambda x: x['id'] == id)['article'][0]
  except:
    return None
  if return_article_only:
    return article
    
  return article, highlight

# articles = get_cnndm_by_id(all_articles['test'], 'dm-test-8764fb95bfad8ee849274873a92fb8d6b400eee2')

# ids of interest
# print(outputs['id'])
print(test_set.columns)

ids = outputs['story_id']

original_documents = []
for id in ids:
  # original_documents.append(get_cnndm_by_id(all_articles['test'], id))
  original_documents.append(get_cnndm_by_id(all_articles['validation'], id))

print(len(original_documents))

# original_documents

"""## Output dataframe

The output dataframe contains the original, deconded, reference and mean annotations by experts in each dimension.
"""

# outputs.drop(['expert_annotations', 'turker_annotations',
#         'model_id', 'filepath', 'e1_coherence', 'e1_consistency',
#        'e1_fluency', 'e1_relevance', 'e2_coherence', 'e2_consistency',
#        'e2_fluency', 'e2_relevance', 'e3_coherence', 'e3_consistency',
#        'e3_fluency', 'e3_relevance'], axis=1, inplace=True)

outputs['original'] = original_documents
print(outputs.head())

print(outputs['original'].iloc[15])

print(outputs['decoded'].iloc[15])

print(outputs['reference'].iloc[15])

"""# Coherence Modeling

## Feature space

We will train a model to classify coherence given the following features

MODEL 1 (All in a SVM) (1600 samples / 80% train and 20% to test)
- $tm_i$: Transition Matrix for document $i$
- $model id$: model type based on 16 different types of extractive and abstractive models.
- $topic \ coverage$: cosine similarity between original text and decoded summary.
- $rouge-3$: overalap of trigrams between golden and decoded summary.

MODEL 2 (LR, DT, NN same as other dimensions) 1600 samples 80% train and 20% to test
- $model id$: model type based on 16 different types of extractive and abstractive models.
- $topic \ coverage$: cosine similarity between original text and decoded summary.
- $rouge-3$: overalap of trigrams between golden and decoded summary.

## Transition Matrix with Egrid

### Setup

Before running our SVM model, first we need to create the transition matix from entity grids.

### Run the Server: CoreNLP
"""

# %cd content/stanford-corenlp-4.3.2/
# !java -mx4g -cp "*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer  -port 9000 -timeout 75000

# construct a CoreNLPClient with some basic annotators, 
# a memory allocation of 4GB, and port number 9001
client = CoreNLPClient(
    annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner','parse','coref'],
    properties={'annotators': 'coref', 'coref.algorithm' : 'statistical'}, 
    memory='4G', 
    endpoint='http://localhost:9001',
    be_quiet=True)
print(client)

!ps -o pid,cmd | grep java

"""### Create Transition Matrices

For each decoded summary we will create a corresponding transition matrix.
The output is a dataframe with transitions as columns $M$ and $d_i$ rows, where $d$ is a document and $i$ is the row index.

Ouput dataframe dimensions $M \cdot d_i$.
"""

# Annotate some text
text = "Albert Einstein was a German-born theoretical physicist. He developed the theory of relativity. Anna was his best friend. She loves drawing trees."
document = client.annotate(text)

# Create Transition Matrix
tm = TransitionMatrix(outputs['decoded'], n=3)

tm.tran_matrix.shape

tm.tran_matrix

# captured the errors from first run and removed them from the a_df
error_ix = [550,630,854,870,1062,1063,1064,1065,1069,1350,1357,1558,1590,1591,1593,1597]
len(error_ix)

# Errors for the validation set
error_ix = [208, 372]
len(error_ix)

"""### PCA for Transition Matrix"""

from sklearn.preprocessing import StandardScaler

X = tm.tran_matrix

# Standardizing the features
X = StandardScaler().fit_transform(X)

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
principalComponents = pca.fit_transform(X)
principalDf = pd.DataFrame(data = principalComponents
             , columns = ['pc1_tm', 'pc2_tm'])

principalDf

# Function to insert row in the dataframe
def Insert_row_(row_number, df, row_value):
    # Slice the upper half of the dataframe
    df1 = df[0:row_number]
  
    # Store the result of lower half of the dataframe
    df2 = df[row_number:]
  
    # Insert the row in the upper half dataframe
    df1.loc[row_number]=row_value
  
    # Concat the two dataframes
    df_result = pd.concat([df1, df2])
  
    # Reassign the index labels
    df_result.index = [*range(df_result.shape[0])]
  
    # Return the updated dataframe
    return df_result

for ix in error_ix:
  print(ix)  
  # Let's create a row which we want to insert
  row_number = ix
  row_value = np.nan
  
  if row_number > principalDf.index.max()+1:
      print("Invalid row_number")
  else:
  
      # Let's call the function and insert the row
      # at the second position
     principalDf = Insert_row_(row_number, principalDf, row_value)
  
      # Print the updated dataframe
      # test_pca

outputs = pd.concat([outputs, principalDf], axis=1)
outputs

"""### Training SVM Model (Section unused)"""

# Remove ix with errors and reset index
X_egrid  = tm.tran_matrix.reset_index(drop=True)
print(X_egrid.shape)

X_model = a_df['model_id'].drop(error_ix, axis=0).reset_index(drop=True)
print(X_model.shape)
X = pd.concat([X_egrid, X_model], axis=1)

# Target variable
y = round(a_df['mean_coherence'].drop(error_ix, axis=0),ndigits=0)

print(X.shape)
print(y.shape)

# First we encode the feature = model_id to numeric
label_encoder = preprocessing.LabelEncoder()
label_encoder.fit(X['model_id'])
X['model_id'] = label_encoder.transform(X['model_id'])

label_encoder.fit(y)
y = label_encoder.transform(y)

X = X.to_numpy()
# y = y.to_numpy().reshape(-1,1)

# Shuffle the dataset
X, y = shuffle(X, y, random_state=0)

# Split into train/test

X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=.3)

X_dev, X_test, y_dev, y_test = train_test_split(X_dev, y_dev, test_size=.35)

print(f"Train: {X_train.shape}, {y_train.shape}")
print(f"Dev: {X_dev.shape}, {y_dev.shape}")
print(f"Test: {X_test.shape}, {y_test.shape}")

# Check target is not continuos
utils.multiclass.type_of_target(y_train)

# Check rank is 1-5
np.unique(y)

param_grid = {'C': [0.01, 0.1, 1, 10,100,1000],
              'kernel': ['linear', 'poly', 'rbf', 'sigmoid']             
}
base_model = svm.SVC()
model = GridSearchCV(base_model, param_grid, scoring='accuracy', cv=5)

# Train the model
model.fit(X_train, y_train)
model.best_params_

# Predict dev data
# y_pred = model.predict(X_dev)
# sorted(model.cv_results_)

# Print the best parameters
print(f'Best parameters: {model.best_params_}')

# Print the bes score
print(f'Best score: {model.best_score_}')

# Print score based on dev data
score = model.score(X_dev, y_dev)
print(f'Test accuracy: {score:.2f}')

# The best estimator
print(f'The best estimator {model.best_estimator_}')

y_preds = model.score(X)
len(y_preds)

# Shut down the background CoreNLP server
client.stop()

time.sleep(10)
!ps -o pid,cmd | grep java

"""## Topic Modeling

### LDA for topic detection

We will create a dictionary using 100 original documents present in our CNN/DM dataset that correspond to the decoded summaries.

This will help us create a vocabulary that it's part of the news data set and from which multiple topics will emerge. Topics are created via unsupervised learning using LDA topic modeling.

There are two main steps 

1. We use unsupervised learning to create topics and represent each document as a vector.
2. We calculate the cosine similarity between original text and decoded text vectors.

The cosine similarity between both vectors should represent how well the summaries topics are represented given the original text topics.
"""

def initial_clean(text):
    """
    Function to clean text of websites, email addresess and any punctuation
    We also lower case the text
    """
    text = re.sub("((\S+)?(http(s)?)(\S+))|((\S+)?(www)(\S+))|((\S+)?(\@)(\S+)?)", " ", text)
    text = re.sub("[^a-zA-Z ]", "", text)
    text = text.lower() # lower case the text
    text = nltk.word_tokenize(text)
    return text

def remove_stop_words(text):
    """
    Function that removes all stopwords from text
    """
    return [word for word in text if word not in stop_words]

def stem_words(text):
    """
    Function to stem words, so plural and singular are treated the same
    """
    try:
        text = [stemmer.stem(word) for word in text]
        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words
    except IndexError: # the word "oed" broke this, so needed try except
        pass
    return text

def apply_all(text):
    """
    This function applies all the functions above into one
    """
    return stem_words(remove_stop_words(initial_clean(text)))

lemmatizer = WordNetLemmatizer()
stop_words = stopwords.words('english')
stemmer = PorterStemmer()

df = pd.DataFrame()
df_decoded = pd.DataFrame()
df_originals = pd.DataFrame()
df_reference = pd.DataFrame()

# docs = train_data['article'][:100]
# # Try with 100 original documents
# df['tokenized'] = outputs['original'].apply(lambda x: apply_all(str(x)))

# Try with 1000 original documents
df['tokenized'] = test_set['article'].apply(lambda x: apply_all(str(x)))
df_decoded['tokenized'] = outputs['decoded'].apply(lambda x: apply_all(str(x)))
df_originals['tokenized'] = outputs['original'].apply(lambda x: apply_all(str(x)))
df_reference['tokenized'] = outputs['reference'].apply(lambda x: apply_all(str(x)))
# apply_all(outputs['original'].iloc[0])
# tokenized = [apply_all(z) for z in docs]

print(df.shape)

all_words = [word for item in df['tokenized'] for word in item]

# use nltk fdist to get a frequency distribution of all words
fdist = FreqDist(all_words)

# Number of unique words
print(len(fdist))

# choose k and visually inspect the bottom 10 words of the top k
k = 60000
top_k_words = fdist.most_common(k)
top_k_words[-10:]

# define a function only to keep words in the top k words
top_k_words,_ = zip(*fdist.most_common(k))
top_k_words = set(top_k_words)
def keep_top_k_words(text):
    return [word for word in text if word in top_k_words]

df['tokenized'] = [keep_top_k_words(z) for z in df['tokenized']]
# df_decoded['tokenized'] = [keep_top_k_words(z) for z in df_decoded['tokenized']]

print(len(df['tokenized']))

df['length'] = df['tokenized'].apply(lambda x: len(x))

# document length

print("length of list:",len(df['tokenized']),
      "\naverage document length", np.average(df['length']),
      "\nminimum document length", min(df['length']),
      "\nmaximum document length", max(df['length']))

print(df)

df.drop(labels='length', axis=1, inplace=True)
# df = df.dropna(axis=0, inplace=True, subset=['tokenized'])

# only keep articles with more than 30 tokens, otherwise too short
df = df[df['tokenized'].map(len) > 40]
# make sure all tokenized items are lists
df = df['tokenized'].map(lambda x: list(x))
df.reset_index(drop=True,inplace=True)
df.head()

df_decoded = df_decoded['tokenized'].map(lambda x: list(x))
df_decoded.reset_index(drop=True,inplace=True)

df_originals = df_originals['tokenized'].map(lambda x: list(x))
df_originals.reset_index(drop=True,inplace=True)

df_reference = df_reference['tokenized'].map(lambda x: list(x))
df_reference.reset_index(drop=True,inplace=True)

len(df)
print(df_decoded)

"""### Unsupervised training"""

np.random.seed(393)
msk = np.random.rand(len(df)) < 0.999

train_df = df[msk]
train_df.reset_index(drop=True,inplace=True)

test_df = df[~msk]
test_df.reset_index(drop=True,inplace=True)

print(len(df),len(train_df),len(test_df))

# Create dictionary and corpus as BOW
dictionary = corpora.Dictionary(train_df)
corpus = [dictionary.doc2bow(doc) for doc in train_df]

TFIDF = models.TfidfModel(corpus) # Fit TF-IDF model
trans_TFIDF = TFIDF[corpus] # Apply TF-IDF model

import logging
            
import numpy as np
from gensim.topic_coherence import direct_confirmation_measure

log = logging.getLogger(__name__)

ADD_VALUE = 1


def custom_log_ratio_measure(segmented_topics, accumulator, normalize=False, with_std=False, with_support=False):
    topic_coherences = []
    num_docs = float(accumulator.num_docs)
    for s_i in segmented_topics:
        segment_sims = []
        for w_prime, w_star in s_i:
            w_prime_count = accumulator[w_prime]
            w_star_count = accumulator[w_star]
            co_occur_count = accumulator[w_prime, w_star]

            if normalize:
                # For normalized log ratio measure
                numerator = custom_log_ratio_measure([[(w_prime, w_star)]], accumulator)[0]
                co_doc_prob = co_occur_count / num_docs
                m_lr_i = numerator / (-np.log(co_doc_prob + direct_confirmation_measure.EPSILON))
            else:
                # For log ratio measure without normalization
                ### _custom: Added the following 6 lines, to prevent a division by zero error.
                if w_star_count == 0:
                    log.info(f"w_star_count of {w_star} == 0. Adding {ADD_VALUE} to the count to prevent error. ")
                    w_star_count += ADD_VALUE
                if w_prime_count == 0:
                    log.info(f"w_prime_count of {w_prime} == 0. Adding {ADD_VALUE} to the count to prevent error. ")
                    w_prime_count += ADD_VALUE
                numerator = (co_occur_count / num_docs) + direct_confirmation_measure.EPSILON
                denominator = (w_prime_count / num_docs) * (w_star_count / num_docs)
                m_lr_i = np.log(numerator / denominator)

            segment_sims.append(m_lr_i)

        topic_coherences.append(direct_confirmation_measure.aggregate_segment_sims(segment_sims, with_std, with_support))

    return topic_coherences

from gensim.topic_coherence import direct_confirmation_measure
direct_confirmation_measure.log_ratio_measure = custom_log_ratio_measure

# Coherence values for varying alpha
def compute_coherence_values_ALPHA(corpus, dictionary, seed, num_topics, texts, alpha, eta, start, limit, step):
    # coherence_values = []
    # model_list = []
    # for eta in range(start, limit, step):
    model = gensim.models.LdaMulticore(corpus=corpus
                                        , id2word=dictionary
                                        , num_topics=num_topics
                                        , random_state=seed
                                        , eta=eta
                                        , alpha = alpha
                                        , passes=20
                                        , workers = 3)
    # model_list.append(model)
    coherencemodel = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
    print("Coherence score c_v: ", coherencemodel.get_coherence())
    # coherence_values.append(coherencemodel.get_coherence())
    # return model_list, coherence_values
    return model, coherencemodel

NUM_topics = [70]
ALPHA = .1
ETA = .1
SEED = 343
START = 1
LIMIT = 2
STEP = 1
results = []

for topic_size in NUM_topics:
  print("Topic size:", topic_size)
  lda, coherence_values = compute_coherence_values_ALPHA(dictionary=dictionary
                                                              , corpus=trans_TFIDF
                                                              , num_topics=topic_size
                                                              , seed=SEED
                                                              , texts=train_df
                                                              , alpha = ALPHA
                                                              , eta = ETA
                                                              , start=START
                                                              , limit=LIMIT
                                                              , step=STEP)
  # results.append([model_list, coherence_values])
  
  # # Plot graph of coherence values by varying alpha
  # limit=LIMIT; start=START; step=STEP;
  # x_axis = []
  # for x in range(start, limit, step):
  #     x_axis.append(x/10)
  # plt.plot(x_axis, coherence_values)
  # plt.xlabel("eta")
  # plt.ylabel("Coherence score")
  # plt.legend(("coherence"), loc='best')
  # plt.show()

"""### Topic cosine similarity"""

def get_topic_distribution_from_document(tokens_from_doc):
  bow = dictionary.doc2bow(tokens_from_doc)
  doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow, minimum_probability=0.0)])
  return doc_distribution

# this is better when comparing two vectors
def cosine_similarity(original, decoded):
  cos_sim = dot(original, decoded)/(norm(original)*norm(decoded))
  print(cos_sim)
  return cos_sim 

topic_similarity_oe= []
topic_similarity_er= []
print(len(train_df))
# # dictionary.add_documents(train_data)
for ix in range(len(df_originals)):
  print(ix)
  
  bow_original = dictionary.doc2bow(df_originals[ix])
  # print(bow_original)

  bow_encoded = dictionary.doc2bow(df_decoded[ix])
  # print(bow_encoded)

  bow_reference = dictionary.doc2bow(df_reference[ix])
  # print(bow_encoded)

  # # get the topic contributions for the document chosen at random above
  original_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow_original, minimum_probability=0.0)])
  encoded_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow_encoded, minimum_probability=0.0)])
  reference_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow_reference, minimum_probability=0.0)])

  # print(original_distribution)
  # print(encoded_distribution)

  cs_original_encoded = cosine_similarity(original_distribution, encoded_distribution)
  topic_similarity_oe.append(cs_original_encoded)

  cs_encoded_reference = cosine_similarity(encoded_distribution, reference_distribution)
  topic_similarity_er.append(cs_encoded_reference)

print(len(topic_similarity_er))

plt.hist(np.log(topic_similarity_oe), bins=50)

plt.hist(topic_similarity_er)

print(min(topic_similarity_er))
print(min(topic_similarity_oe))

outputs['top_sim_original_encoded'] = topic_similarity_oe
outputs['top_sim_encoded_reference'] = topic_similarity_er
outputs

# vectors = []
# for ix in range(len(train_df)-1):
#   topic_vector = get_topic_distribution_from_document(train_df.iloc[ix])
#   vectors.append(topic_vector)

# # we need to use nested list comprehension here
# # this may take 1-2 minutes...
# doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lda[corpus]])
# doc_topic_dist.shape

# # # Distribution of topics sums to 1
# doc_distribution.sum()

"""# Export data"""

import datetime
from google.colab import files

def write_to_csv(df, inner_filename):
  now = datetime.datetime.now()
  filename = now.strftime("%Y-%m-%d-%H-%M-%S")

  compression_opts = dict(method='zip', archive_name='{}.csv'.format(inner_filename))

  outputs.to_csv('{}.zip'.format(filename), index=False, compression = compression_opts)
  files.download('{}.zip'.format(filename))

write_to_csv(outputs, inner_filename='data_scored_with_top_tranmatrix')

"""# Get full texts"""

import pickle

with open('corpus.pkl', 'wb') as f:
  pickle.dump(corpus, f)