# -*- coding: utf-8 -*-
"""SummAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fglU47Dta73UrMoHAFdtINAooKpeRmd4

# NLP With Deep Learning (W266)

Submission by *Carolina Arriaga, Ayman, Abhi Sharma*

Winter 2021 | UC Berkeley

## Notebook Overview

This notebook contains the variable summary length analysis done by the team on dimension scores

# Utils
"""

import datetime
from google.colab import files
from os import listdir
from os.path import isfile, join

def write_to_csv(df, inner_filename):
  now = datetime.datetime.now()
  filename = now.strftime("%Y-%m-%d-%H-%M-%S")

  compression_opts = dict(method='zip', archive_name='{}.csv'.format(inner_filename))

  df.to_csv('{}.zip'.format(filename), index=False, compression = compression_opts)
  files.download('{}.zip'.format(filename))

def download_all_imgs():
  imgs = [f for f in listdir('/content') if isfile(join('/content', f))]
  imgs = [p for p in imgs if p.endswith('png') or p.endswith('jpg') or p.endswith('jpeg')]
  print(imgs)
  for p in imgs:
    files.download(p)

"""# Data Prep"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import xgboost
import os
import matplotlib.pyplot as plt
import copy

# load data, create X, y train test
train_df = pd.read_csv('analysis_shap_analysis.csv')
X_train = train_df.drop(['id', 'model_id', 'model_variant', 'expert_coherence_avg', 'expert_consistency_avg', 'expert_fluency_avg',
             'expert_relevance_avg', 'top_sim_original_encoded', 'top_sim_encoded_reference'], axis=1)
info_train = train_df[['id', 'model_id', 'model_variant']]


test_df = pd.read_csv('data_scored_with_top_tranmatrix.csv')
info_test = test_df[['story_id', 'model_id', 'model_variant', 'decoded', 'reference', 'expected_token_count_decoded', 'word_count_decoded', 'word_count_reference']]
test_df_x_cols = [c for c in test_df if c in list(X_train.columns)]
assert(len([c for c in list(X_train.columns) if c not in test_df_x_cols])) == 0
assert (sorted(test_df_x_cols) == sorted(list(X_train.columns)))

"""# Model Prep and Run

"""

def get_model(X, y, dim):
    # train test split
    X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.8, random_state=42)

    # create model
    model = xgboost.XGBRegressor(objective='reg:squarederror', max_depth=2, learning_rate=0.1,
                                 n_estimators=6000, n_jobs=15, base_score=np.mean(y_train), subsample=0.7,
                                 colsample_bytree=0.9, reg_lambda=50)

    # fit model
    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_validation, y_validation)],
              eval_metric=['rmse', 'mae'], early_stopping_rounds=200, verbose=False);

    print('For model dimension: {}, Validation RMSE: {} '.format(dim, np.sqrt(mean_squared_error(y_validation, model.predict(X_validation)))))
    print('For model dimension: {}, Validation MAE: {}'.format(dim, np.sqrt(mean_absolute_error(y_validation, model.predict(X_validation)))))

    return model

model_coherence = get_model(X_train, train_df['expert_coherence_avg'], dim='coherence')
model_fluency = get_model(X_train, train_df['expert_fluency_avg'], dim='fluency')
model_consistency = get_model(X_train, train_df['expert_consistency_avg'], dim='consistency')
model_relevance = get_model(X_train, train_df['expert_relevance_avg'], dim='relevance')

"""# Results and Analysis"""

# column order matters and must match the order with which the model was trained
# https://stackoverflow.com/questions/42338972/valueerror-feature-names-mismatch-in-xgboost-in-the-predict-function
test_df_x_cols = model_coherence.get_booster().feature_names
assert (sorted(test_df_x_cols) == sorted(list(X_train.columns)))

coherence_pred = model_coherence.predict(test_df[test_df_x_cols])
fluency_pred = model_fluency.predict(test_df[test_df_x_cols])
consistency_pred = model_consistency.predict(test_df[test_df_x_cols])
relevance_pred = model_relevance.predict(test_df[test_df_x_cols])

# clip values to be between 1 and 5 - though not strictly correct, but we need a way to convert back to likert scale
def get_correct_value(val):
  if val <=1:
    return 1
  if val >=5:
    return 5
  return val

coherence_pred = [get_correct_value(p) for p in coherence_pred]
fluency_pred = [get_correct_value(p) for p in fluency_pred]
consistency_pred = [get_correct_value(p) for p in consistency_pred]
relevance_pred = [get_correct_value(p) for p in relevance_pred]

test_df['coherence_pred'] = coherence_pred 
test_df['fluency_pred'] = fluency_pred 
test_df['consistency_pred'] = consistency_pred 
test_df['relevance_pred'] = relevance_pred 
write_to_csv(test_df, "data_scored_predicted")

"""## Distributions

### Overall
"""

import matplotlib.pyplot as plt
plt.figure(figsize=(5, 5))
plt.hist(coherence_pred)

import matplotlib.pyplot as plt
plt.figure(figsize=(5, 5))
plt.hist(fluency_pred)

import matplotlib.pyplot as plt
plt.figure(figsize=(5, 5))
plt.hist(consistency_pred)

import matplotlib.pyplot as plt
plt.figure(figsize=(5, 5))
plt.hist(relevance_pred)

"""### Extractive vs Abstractive"""

extractive = ['topN', 'tfidf_vec', 'cossim_rank', 'bert_ext']
abstractive = ['bart', 't5', 'pegasus', 'gpt3']

def get_model_type(model_name):
  if model_name in extractive:
    return "extractive"
  if model_name in abstractive:
    return "abstractive"
  else:
    raise Exception(model_name) 

assert(sorted(list(test_df['model_id'].unique())) == sorted(extractive + abstractive))

test_df['model_type'] = test_df.apply(lambda row : get_model_type(row['model_id']), axis = 1)

test_df['coherence_pred'].hist(by = test_df['model_type'], sharex=True)

test_df['fluency_pred'].hist(by = test_df['model_type'], sharex=True)

test_df['consistency_pred'].hist(by = test_df['model_type'], sharex=True)

test_df['relevance_pred'].hist(by = test_df['model_type'], sharex=True)

"""## Sample BAD Summaries"""

def get_sample_rows_below_dim(df, dim, dim_threshold=3, num_rows=1):
  for idx, row in df.loc[df[dim] < dim_threshold].sample(n=num_rows).iterrows():
    print_row(row)

def get_sample_rows_above_dim(df, dim, dim_threshold=4.5, num_rows=1):
  for idx, row in df.loc[df[dim] >= dim_threshold].sample(n=num_rows).iterrows():
    print_row(row)

def print_row(row):
  print("REF: {}".format(row['reference']))
  print()
  print("DECODED: {}".format(row['decoded']))
  print()
  print("MODEL: {}-{}".format(row['model_id'], row['model_variant']))
  print("####################################################################")
  print()

get_sample_rows_below_dim(test_df, 'coherence_pred', dim_threshold=3, num_rows=1)

get_sample_rows_below_dim(test_df, 'fluency_pred', num_rows=1, dim_threshold=3.5)

get_sample_rows_below_dim(test_df, 'consistency_pred', num_rows=1, dim_threshold=3.5)

get_sample_rows_below_dim(test_df, 'relevance_pred', num_rows=1, dim_threshold=2.5)

"""## Sample GOOD Summaries"""

get_sample_rows_above_dim(test_df, 'coherence_pred', num_rows=1)

get_sample_rows_above_dim(test_df, 'fluency_pred', num_rows=1)

get_sample_rows_above_dim(test_df, 'consistency_pred', num_rows=1, dim_threshold=5)

get_sample_rows_above_dim(test_df, 'relevance_pred', num_rows=1, dim_threshold=4.2)

"""## Scores Over Varying Length Summaries"""

scored_df = test_df[['model_id', 'model_variant', 'word_count_decoded', 'coherence_pred', 'fluency_pred', 'consistency_pred', 'relevance_pred']]

import matplotlib.pyplot as plt
from scipy.interpolate import make_interp_spline
from scipy.interpolate import interp1d
import numpy as np
from scipy.interpolate import make_interp_spline, BSpline
from matplotlib.pyplot import figure

def get_plots(df, dim_col, model_type='extractive', only_model=None, save_img=False):
  assert dim_col.endswith('_pred')
  models = extractive
  if model_type == 'abstractive':
    models = abstractive

  # override the models to evaluate  
  if only_model is not None:
    assert only_model in extractive + abstractive
    models = [only_model]

  # set figure size
  plt.figure(figsize=(10, 10))
  for m in models:
    variants = list(df.loc[df['model_id'] == m]['model_variant'].unique())
    for v in variants:
      df_selected = df.loc[(df['model_id'] == m) & (df['model_variant'] == v)]
      df_sorted = df_selected.sort_values(['word_count_decoded'], ascending = (True))
      y = np.array(df_sorted[dim_col])
      x = np.array(df_sorted['word_count_decoded'])

      # perform smoothing to plot on graph
      # https://stackoverflow.com/questions/68100959/plot-smooth-curve-with-duplicate-values-in-list
      param = np.linspace(0, 1, x.size)
      spl = make_interp_spline(param, np.c_[x,y], k=5)
      X_, Y_ = spl(np.linspace(0, 1, x.size * 100)).T
      
      plt.plot(X_, Y_, label = "{}-{}".format(m,v))

  plt.legend()
  # saving only allowed in single model mode
  if save_img and only_model is not None:
    plt.savefig('{}-{}.png'.format(only_model, dim_col.replace('_pred','')), dpi = 300)
  else:
    plt.show()

get_plots(scored_df, 'consistency_pred', only_model='t5')

# get plots for all extractive models for relevance
get_plots(scored_df, 'relevance_pred')

dims = ['consistency', 'fluency', 'coherence', 'relevance']
for d in dims:
  for m in extractive + abstractive:
    get_plots(scored_df, '{}_pred'.format(d), only_model=m, save_img=True)

# zip and download images
!zip images.zip /content/*.png

