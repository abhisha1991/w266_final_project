# NLP With Deep Learning (W266)

Submission by *Carolina Arriaga, Ayman, Abhi Sharma*

Winter 2021 | UC Berkeley

## TLDR-Eval - A New Framework for Summarization Evaluation

Summarization is a hard to evaluate NLP task. A number of standard metrics such as ROUGE-n are available for evaluation of summaries but they are not adequate in capturing different dimensions of interest (fluency, consistency, coherence, relevance) when evaluating summaries. Our project explores a number of different suggested metrics in literature for comprehensive evaluation and attempts to map these metrics against one or more of these dimensions. We attempt to explore the attribution of each of these metrics in determining the signal it projects on the dimension of interest. We finally apply this framework of evaluation to a number of generated summaries from different abstractive and extractive models - to show how our suggested framework is able to score model summaries adequately.

### References:

#### Main
1. https://arxiv.org/pdf/2007.12626.pdf
2. https://github.com/Yale-LILY/SummEval

#### Misc
1. https://www.ijcai.org/Proceedings/05/Papers/0505.pdf
2. https://github.com/danieldeutsch/sacrerouge/blob/master/doc/metrics/pyreval.md

